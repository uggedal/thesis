\section{Discussion}

This section will discuss our various research questions in relation
to the results we have presented. We'll start with looking at
several aspects of activity streams as a social navigation
mechanism before we discuss prototyping with Greasemonkey on
an established web site.

When presenting our results we worked with a level of significance
$\alpha$ of $p \leq 0.05$%
\sidenote{
  See \sectionref{empirical.method.data.analysis.level.of.significance}
  for details.
}
and handled values of $p$ in a black and white fashion.
\prequote[\p{1277}]{rosnow89}{argues that}{%
  surely, God loves the .06 nearly as much as the .05}
We are therefore going to discuss results with values of $p$ approaching
$\alpha$ in a more flexible fashion in this section.

Based on our profiling of the respondents (see 
\tablepageref{respondents.profile.usage} for details)
we found the two groups which were using our implementation to be
representative of the general sample in all aspects except how
frequent they used \urort{}. The control group used \urort{} more
frequent than than both the general sample and the experiment group
with a probability of 0.055 compared to the general sample.
We argue that this makes the control group more experienced
\urort{} users than the experiment group.
This is an important aspect which we'll try to keep in mind in
the discussion about activity streams which follows.

\subsection{%
  Can social navigation trough activity streams help users keep
  up-to-date on favorites' activities on \urort{}?
}

We hypothesized that usage of activity streams would improve the level
of which participants could keep up-to-date on what their favorites
on \urort{} were doing.

\subsubsection{Activities in general}

Comparing how easy respondents felt it was to keep up-to-date on activities
within the experiment and control group%
\sidenote{
  See \tablepageref{up.to.date.favorite.activities.within} for details.
}
we observed a higher increase in agreement for the experiment group. The
increase of agreement for the control group could be explained as
a placebo effect.
Since the probability of making a type I error are
$p = 0.086$ we simply take this result as an indication and not as hard proof
for validating our alternative hypothesis.

In addition the comparison within
groups we also compared the differences between the two groups. We observed
similar positive results for activity streams with $p$ approaching $\alpha$
with 0.089.
We take the higher increase of agreement for the experiment
group as an indication of the appropriateness of an activity stream for keeping
up to date on activities. We will not generalize these results to say that
activity streams are uniformly appropriate for keeping up-to-date on
favorites.

\subsubsection{Specific activities}

When asking participants to qualify more specific statements of how
easy they felt they could keep up-to-date on different types of activities
we observed differences between%
\sidenote{
  See
  \tablepageref{up.to.date.favorite.specific.activities.between}
  for details.
}
the groups and within the groups%
\sidenote[2]{
  See
  \tablepageref{up.to.date.favorite.specific.activities.within}
  for details.
}.
The between group data showed no significant nor borderline significant
differences between groups. This contradicts the borderline significant
results we found with the same comparison on activities in general.
When comparing within groups the only notable and significant difference
appeared for activities relating to publishing new blog posts.

Why do users of activity streams so strongly feel that the stream help them to
better keep up-to-date on recent blog posts and not the other types of
activities? One possible explanation could be that the activity stream
shows an excerpt of blog posts after the blog author and title.%
\sidenote{
  The activity stream with excerpts can be seen in
  \figurepageref{scrsh.urort.activity.feed}.
}
This means
that the user can get a glimpse into the content of the blog post without
actually navigating to the post itself.
But reviews from other users are also displayed with an excerpt of its
content. We therefore find this explanation to be highly suggestive.

Our data shows no indicators as why we experienced contradicting results
when asking how easy users could keep up-to date on activities in general
compared to specific activities. We note this as a potentially useful lesson
when conducting questions about the ease of conducting a task. In our case
asking more generally gave larger differences than asking more specific
questions.

\subsubsection{Perceived usefulness}

We tried to characterize in what way activity streams could be better than
no such feature for keeping up-to-date on favorites by asking respondents
to gauge statements of perceived usefulness.%
\sidenote{
  The results are available in
  \tablepageref{up.to.date.favorite.perceived.usefulness.between}.
}
The results of asking in this manner yielded no noticeable nor significant
differences between experiment and control respondents. Similarly to asking
for specific activities, asking for specific qualities of keeping up-to-date
seems to result in only minor differences between groups.

\subsubsection{Perceived ease of use}

Our investigation into the perceived ease of use%
\sidenote{
  The results are available in
  \tablepageref{up.to.date.favorite.perceived.ease.of.use.between}.
}
showed only minor differences between experiment and control groups. On the
issue of how flexible the prototype was respondents from the control group
actually reported higher acceptance to our statements.

We think the reason for
this lies in the nature of asking respondents about ease of use without
mentioning for what task the prototype should be easy to use. Our placebo
prototype had fewer features (lacking activity streams) and was more simple
than the experiment prototype with an activity stream. Surely the simplest
application would be easier to use as there are less information and less
navigational possibilities available. The statements of perceived ease of use
investigates the overall ease of using the application, not how easy it to use
for keeping up-to-date on activities. They measure technological acceptance
and not actual acceptance of the application to perform a particular task.

\subsubsection{Acceptance as standard feature}

Relating to perceived ease of use and technological acceptance is our question
about whether respondents wanted the prototype to be a standard feature
on \urort{}. The results%
\sidenote{
  \tablepageref{up.to.date.standard.feature.between}
  lists the results.
}
indicated a dead race between the prototype with an activity feed and the
prototype without. While this data does not show a higher acceptance of
activity feeds its interesting to not how high the acceptance are for
our respondents. Even the placebo\dash{}composed of only a list of
a user's favorites\dash{}seems to be so useful for respondents that they want
it included together with \urort{}'s standard features.

\subsubsection{Activity streams for keeping up-to-date on favorites}

Based on our data we've seen that its still unconclusive whether activity
streams help users in keeping up-to-date on activities. There seem to be an
indication of the usefulness of activity streams in this regard.
Activity streams clearly makes the task of keeping up with blog posts on
\urort{} easier.

We've seen how
one asks respondents questions can make a difference in the results one
are able to obtain. In our case more specific questions yielded answers
further from where we believed them to be based on our hypothese. A more
general question resulted in data more in line with our expectations.

\subsection{%
  Does social navigation trough activity streams lead users to more often keep
  up-to-date on favorites' activities on \urort{}?
}

We hypothesized that usage of an activity stream would result in higher
frequencies of keeping up-to-date on favorites' activities on \urort{}.

\subsubsection{Keeping up-to-date frequency}

As we indicated in the beginning of our discussion the control respondents
seemed more experienced with using \urort{}. This is evident in how
frequent they keep up to date compared to experiment respondents for the
posttest.%
\sidenote{
  See
  \tablepageref{up.to.date.favorite.activities.frequency.between}
  for details.
}
The control group's more frequent action of keeping up-to-date is highly
significant. When we compared the shift in the frequency of keeping up-to-date
from the pretest to the posttest%
\sidenote{
  See
  \tablepageref{up.to.date.favorite.activities.frequency.within}
  for details.
}
we notice that the control group's frequency is practically unchanged.
Interestingly the experiment respondents frequency of keeping up-to-date have
increased significantly over the same period. This is a good example
of how our pretest and posttest experiment design have enabled us to look
at change over time within groups without jumping to inconclusive inferences
by looking only at the state after the treatment or placebo was introduced.

\subsubsection{Prototype usage frequency}

Our data concerning how often the prototype was used between the two groups%
\sidenote{
  See
  \tablepageref{up.to.date.prototype.frequency.between}
  for details.
}
are different than our findings of how often the same groups kept up-to-date
on favorites' activities. The prototype usage data showed that the experiment
group had a higher usage rate of \latest{} than the control respondents. The
results approaches $\alpha$ with $p = 0.056$.

Does this mean that these two data sources contradict eachother? Not
necessarily. The first frequency of use statistics shows how often the two
groups kept up-to-date on activities while the second shows how often
the two groups used our prototype. We believe the lower control group usage of
the prototype is related to the lower usefulness of the placebo
implementation.

Why do the control group then report higher frequencies for keeping up-to-date
on activities? There could be several reasons, but we believe this could
indicate that the control grup are keeping up-to-date on favorites' activities
with other means than the placebo prototype implementation. This could then
indicate that the prototype without an activity stream is less useful
for keeping up-to-date on activities.

\subsubsection{Keeping up-to-date more often with activity streams}

To summarize, we believe that an activity stream makes respondents
more often keep up-to-date on activities than without. Those having used such
a tool reported larger changes over time in how frequent they conducted such
tasks than those which did not. When one takes into account how often the
prototype implementation was used it seems like the prototype without
an activity stream is less suitable for keeping up-to-date than that which
implements such a feature. Respondents without an activity stream would
then need to keep up-to-date on activities with other means than solely
relying on the prototype.

\subsection{%
  Does social navigation trough activity streams lead users to make
  more artists on \urort{} their favorites?
}

We hypothesized that usage of activity streams on \urort{} would
make respondents add more artists to their list of favorites.

As our data%
\sidenote{
  See
  \tablepageref{up.to.date.prototype.frequency.between}
  for details.
}
showed we found no evidence whatsoever for our claims of increases in
number of favorites after having used an activity stream.
We believed that the increased focus on favorites and their activities through
an activity stream would lead users to make more artists their favorites.

One reason for this mismatch between our hypotheses and the evidence could be
that there is no explanation of the benefits of adding favorites on \urort{}.
When a user is browsing the profile page of an artist there is no statement
along the lines of: \q{become a favorite of this artist and get automatically
updated on their latest activities}. Such a reminder of one benefit of adding
artists as favorites could possibly lead to more favoring.

\subsection{%
  Can navigational prototyping with Greasemonkey be considered a
  viable technical option when testing user behavior in an
  established web site?
}

When we conducted the experiment of our prototype with real world users
we got valuable feedback on how well such a technical solution works.
Since Greasemonkey enabled client with a dedicated server backend is (to our
knowledge) a new way of experimenting with social navigation we did not
make any hypotheses about the failure or success of the solution.
We consider this early work on such prototypes where a few lessons
were learnt.

\subsubsection{Limited in browser selection}

As descibed in
\sectionref{selection.stack.client.platform} there exists Greasemonkey-like
implementations for all major browsers. Greasemonkey for Firefox is the only
implementation that supports sending requests to other domains than the domain
a user-script is running under. This means that if one have to use a server
backend to handle the heavy lifting (as we did) one are limited to
using Firefox as a browser plattform.%
\sidenote{
  Unless one have the opportunity to run the server software under the domain
  of the web site one are prototyping. We had no such luxury.
}

Targeting only Firefox as a plattform means that one are unable to reach
all potential users. The ammount of Firefox users lies somewhere
between one tenth and one quarter of all web citizens.%
\sidenote[2]{
  The ammount of users which use Firefox can vary
  between different populations. According to \citet{onestat08} 13.8\% of
  users world wide used Firefox in February 2008. The market share for Firefox
  in Europe lied on 19.7\% in February 2008 according to \citet{adtech08}.
  \citet{xiti08} reports the usage to be 28.8\% for Europe in March 2008
  while 20.3\% for Norway (our candidate country) for the same period.
}
Installing a new web browser to take part in an
experiment is a lot to expect from end-users. One therefore have to take into
account the limited outreach Firefox have when deciding on a Greasemonkey
based prototype.

We specifically asked for users of Firefox when we contancted
789 potential respondents. 171 responded to our request while 123 completed
our pretest survey.%
\sidenote[7]{
  See
  \figurepageref{fig.experiment.mortality}
  for details.
}
Those who completed the pretest survey reported a median usage frequency as
\q{always}.%
\sidenote[8]{
  See
  \tablepageref{respondents.profile.usage}
  for details.
}
This means that from 789 people 171 (22\%) bothered to take our survey and was
likely a Firefox user.
This means that either almost 100\% of all potential respondents with a
Firefox web browser decided to answer our survey (highly unlikely) or Firefox
usage for our sample of \urort{} users are above the norm for Norway.

\subsubsection{Difficulties with installing Greasemonkey and user-scripts}

Installing our prototype software consisted of two steps:

\begin{enum}
  \item Installing the Greasemonkey Firefox extension.
  \item Installing our protoype user-script for Greasemonkey.
\end{enum}

The first step in the installation process consists of
\begin{inparaenum}[(i)]
  \item navigating to the Greasemonkey installation page,
  \item clicking an installation button on the page,
  \item clicking on another installation button in an installation
    dialog, and
  \item restarting the Firefox browser.
\end{inparaenum}
The last step in the installation process consists of
\begin{inparaenum}[(i)]
  \item filling out an email address so we can associate answers
    to the pretest with the group the respondent is randomly distributed to,
  \item pressing an install hyperlink on the user-script installation page,
    and
  \item clicking on an installation button in a user-script installation
    dialog.
\end{inparaenum}
It's evident that this multi-step process can be a bit complicated for the
average user. We tried to make the seperate steps as seamless as possible
by providing screen dumps with descriptions for all parts of the process.

Our non-accomplish rates%
\sidenote{
  See
  \figurepageref{fig.experiment.mortality}
  for details.
}
shows that of the 71 ($E_1 + C_1$) respondents which bothered trying
installing the prototype only 45 ($E_2 + C_2$) managed to do so. This equates
to a drop off rate of 36.6\%.

If we look at our follow-up questionnaire responses%
\sidenote{
  Found in
  \tablepageref{prototype.installation.success}.
}
the drop off rate lies at 23.7\%. This discrepancy probably shows that
frustrated respondents which did not manage to install the software were less
inclined to take a follow-up survey.

These are quite high rates of unsuccessfull
installations and one should keep this in mind when one designs an
experiment where respondents have to install Gresemonkey and user-scripts
themselves. To solve this problem one could design an experiment where
participants were invited to a lab where all software were pre-installed. A
lab study have its shortcomings too, as we've described in
\sectionref{empirical.methodology.experiment.design}.

\parabreak

During our development of a prototype application with Greasemonkey for
enhancing an established web page we got a feel for its pros and cons from a
development perspective.


\subsubsection{Unintrusive for the established implementation}

Prototyping with Greasemonkey is unintrusive for both the creators
of a web site and its users.  One are only manipulating
pages on an already existing web site when having explicitly installed
Greasemonkey and an accompanying user-script. This means that one can
conduct experiments by altering a web site without contacting its authors
nor having to inform the web site's existing users. One are in other words
only altering content on the client side\dash{}in the browser. Normal users
are presented with the web site as served by its the web server.

We had contact with the creators of \urort{} relating to our prototype
implementation. But the prototype could just aswell be implemented without
having contact with the creators.%
\sidenote{
  We benefited from such contact mainly since we were able to easier
  getting hold of experiment participants.
}

\subsubsection{Requires no access to the established implementation}

The creators of \urort{} benefited from a Greasemonkey approach since they
could keep going on with their work without worrying about potential breakage
caused by our prototype. Since we did not access their implementation we could
not make any harm on the product that was delivered to non-experiment users.

\subsubsection{Requires little knowledge of the established implementation}

If we had been permitted access to the \urort{} implementation we imagine
there would be a large upfront investment in learning how the \urort{}
architecture worked before we could start conducting any implementation work.
With a Greasemonkey approach there was some learning which had to be completed
before we could dive in to implementation work, but we believe this to be less
than what would be required to change the \urort{} implementation.

\subsubsection{Could require more work than altering the established
  implementation}

In our work with creating a navigational prototype we had to investigate how
\urort{} worked in its outward facing interface. This was a somewhat
convoluted experience.
% go on to say that if one knows the established implementation well,
% it's probably easier to change that.

% elaborate
\subsubsection{Fragile when the established implementation is changed}
% Only happened once during a two month span on \urort{}. What was scary
% thought was that the changes made the user script on the client side
% obsolete. If this had happened under production usage when the user scripts
% was pushed to the clients we would be in a world of trouble. Changes on the
% server side platform can be handled more transparently.

\subsubsection{Less performant than the established implementation}

\section{Generalizability and Validity}

\subsection{Scale of experiment}

% small number of participants

\subsection{Selection of subjects}

% We were only concerned with Firefox users. These are probably
% not representative for Urørt users.

\subsection{Technical seeding}

% The installation process was somewhat complicated. This means
% that the most technically knowledgeable people are those
% which answered our posttests.

\subsection{Motive for participation}

% The people who want to participate in such an elaborate study is
% probably fairly interested in Urørt. They are experienced Urørt users
% and are therefore not representative of the Urørt population.

\subsection{Implications}

% This has consequences for the validity of our study. We can not generalize
% back to Urørt users in general.
