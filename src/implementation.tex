\chapter{Implementation}
\label{chapter:implementation}

% This chapter should include design choices for my implementation. For
% example choices taken for generating relevant data for test users
% and a computational sound method for doing so. As JavaScript in browsers are
% quite inefficient it will probably be necessary to persist data at a server
% in some sort of cache. The clients could then get this data by invoking a
% single request. The result could for instance be JSON serialized. Scraping
% of such data is probably done more efficient and safer at the server-side
% since multiple XMLHttpRequest in the clients for scraping and parsing could
% prove to be quite computational expensive.

As we've seen in 
\sectionref{building.on.top.of.the.web}
it's possible to build applications on top of existing web sites by creating
transparent prototype implementations. This chapter starts with an account of
what kind of navigation system we wanted to build, goes on to describe why we
decided on such navigational designs, and concludes with an explanation of the
deeper technical decisions we had to make. The following chapter,
\chapterref{selection.of.third.party.software},
describes what kind of third party software we used for realizing the
implementation details we describe in this chapter.


\section{Design}

Our philosophy when creating a navigational design have been
similar to that practiced by
\begin{fullquotation}[\chap{3}]{exupery67}{in aviation design}
  \noindent
  And now, having spoken of the men born of the pilot's craft, I shall say
  something about the tool with which they work, the airplane Have you
  ever looked at a modern airplane? Have you followed from year to year
  the evolution of its lines? Have you ever thought, not only about the
  airplane, but about whatever man builds, that all of man's industrial
  efforts, all his computations and calculations, all the nights spent
  over working draughts and blueprints, invariably culminate in the
  production of a thing whose sole and guiding principle is the ultimate
  principle of simplicity?

  It is as if there were a natural law which ordained that to achieve this
  end, to refine the curve of a piece of furniture, or a ship's keel, or
  the fuselage of an airplane, until gradually it partakes of the
  elementary purity of the curve of a human breast or shoulder, there must
  be the experimentation of several generations of craftsmen. In anything
  at all, perfection is finally attained not when there is no longer
  anything to add, but when there is no longer anything to take away,
  when a body has been stripped down to its nakedness.
\end{fullquotation}

This philosophy is closely related to minimalism, a movement where the
infamous architect Mies van der Rohe popularized the concept of
\q{less is more}\dash{}achieving the maximum effect with the minimum of
means \citep{whitman69}.


% The design is in a way a recommendation system or could be made into one
% based on how the activity is filtered.
%
% no new data. making existing data more readily available.
%
% try to say something about how less data makes choices easier
% to make -- something a interaction feed will try to solve
%
% try to cite and include anecdotes from
% the paradox of choice: why more is less
%
% You know you've achieved perfection in design, not when you have nothing
% more to add, but when you have nothing more to take away.
%   -- Antoine de Saint Exupery (The Little Prince)

\section{Process}

\subsection{Prototyping}

A \term{prototype} is an early version of an application that are used for
finding out more about the problem at hand and it's possible solutions
\citep[\p{409}]{sommerville06}.
The software developed for our research on social navigation fits these
characteristics. It's not supposed to be used after it's behavior is
evaluated. For that it's to inefficient and relies on specific web browser
environments and extensions. This does not mean that the prototype can't have
impact on how \urort{} evolves in the future. If our evaluation favors our
design decisions the developers of \urort{} might take advantage of such
potential improvements in their web site design.

\citet[\pp{409}{410}]{sommerville06} explains that a prototype can be used for
\begin{inparaenum}[(i)]
  \item gathering more sound requirements from users during a
    requirements phase,
  \item evaluating the feasibility of a proposed design during a
    design phase, and
  \item testing the final system by verifying it against the prototype.
\end{inparaenum}
We're following his second example of prototype usage by making a prototype
for what we believe to be a sound social navigation design. This is then
evaluated. If time permitted (sadly one has limited time and resources
available during master thesis research) the results of such an evaluation
could be input in a new design process and a new prototype system.

As \citet[\p{114}]{mcconnell04} explains prototyping can mean different things
based on context. Often it's used to explain systems where one writes the
least amount of code to get a solution and throw it all away when the design
question is answered. This is not our intention. We'll try to make the system
fully operational and make the code we author comprehensible and
valid. More precisely we're creating a \term{high-fidelity} prototype
\cite[\p{78}]{rudd96} with a robust architecture.

The reason to create a robust application is twofold. Firstly we're developing
the prototype as part of our master thesis and it's meant to show how
proficient we are in both programming and designing applications. We
imagine delivering a barely working and poorly constructed software package
would not be beneficial for our examination results.
Secondly the application could potentially see some stress under user testing
depending how large the test population is. Having a crashing or
non-functioning application during such testing could prove to be costly.

We're therefore creating a prototype to demonstrate our ability (or inability)
to develop software. This is not our sole reason, and probably not the most
important. As \citet[\p{292}]{mayhew90} describes prototyping is not
successful if one have not learned something during the process about what one
set out to investigate at the outset. We want to investigate navigational
designs an the prototype is with such a focus nearly a means to an end.

\subsection{Testing}

One can verify that the application one are bulding functions as it's supposed
to do as code is added and changed by using automated tests.
We have personally experienced benefits with using automated testing on
earlier projects.
\term{Test-driven development} is a development technique
where one writes an automated test for a non-existent feature or
improvement before one actually implement the feature itself\dash{}development
is literally driven by tests. So the focus is not primarily on the test, but
to better design software trough a test-first approach.
\citet{janzen07} conduceted studies
on both developers in companies and university students for finding out
wheter a test-driven approach improved the quality of software design.
Groups which wrote a test before the accompanying code were measured
against groups which wrote their test after implementing the solution.
They found that code size decreased when a test-first approach was
used\dash{}both classes and methods were smaller and simpler. In addition
test-first developers had better coverage of the implementation code in
their tests \citep[\p{81}]{janzen07}. This could indicate that it's
easier to choose not to implement tests after you have a piece
of working code. The dicipline that test-driven development dictates
makes it impossible not to write tests.

\term{Behavior-driven development} is a response to the test-driven
development approach introduced by \citeauthor{north06} for shifting the focus
from writing tests to writing specifications of behavior
\citep{north06}. By writing specifications one is able to
more clearly describe the intent of an application than when one
are writing traditional tests.

We are convinced of the benefits of behavior-driven development based on
earlier development projects and are therefore developing our
prototype application with such a development process. Having specifications
that can be automatically run to check how our application conforms to our
expectations is very valuable. This is especially so when we're working
with \urort{} as an external data source since we don't have control over
the stability of it's structure.

\section{Architecture}

Our implementation basically needs to do two things:

\begin{enum}
  \item Collect existing data from various places on the \urort{} web site.
  \item Display this data in existing web pages on the \urort{} web site in
    a way that we hope will enhance navigation.
\end{enum}

\subsection{Extending an Established Web Site}

As we've described earlier in this chapter we're creating a prototype
application. Inspired by solutions in the Hoodwink.d community we set
out to implement our system on top of already established web pages as
described in \sectionref{building.on.top.of.the.web}.

\citet{laird07} makes a case for using the means of such a browser
extension\dash{}Greasemonkey\dash{} and custom scripts for
realizing projects that without such technology would
never have come to existence. As he describe there is a sweet spot where such
an approach really shines. He therefore created a framework for determining
if a given project embodies the factors that would make this kind of an
implementation a particularly suitable solution. What follows is a recitation
of his selection factors and how our project adheres to these.

\subsubsection{Do you have access to the source code of the web application?}

When the source code of the target web site is available it would probably
be easier to just change that. The case is made for using Greasemonkey when
the source code is not available. Since we don't have access to the internals
of \urort{} we see this factor as favorable for Greasemonkey in our project.

\subsubsection{If the application is under your control and source code is
  available, is updating the application a risky endeavor?}

This factor does not apply to our implementation since we obviously did not
create \urort{} ourselves. Had we done that we could have used
Greasemonkey for adding new features without putting the existing web
site in danger as it would be externalized from the original implementation.

\subsubsection{How critical is the feature to be added?}

If the web site is not functional or complete without the new feature
it is advisable to defer from using Greasemonkey. The reason is the
difficulty of ensuring that all users have the extension installed
and the newest version of the custom script. Since we will be able
to ensure that our test users have the right extension and scripts
installed this is of no concern to us. In addition \urort{} is
functioning fine without our feature enhancement which makes
Greasemonkey a sound technical solution for our means regarding this factor.


\subsubsection{Is Firefox available for the potential users, and does it
  work with the target application?}

The Firefox web browser should be available to install on most platforms
and we can report that it works fine on the target web site.

\subsubsection{To what degree are the target users computer literate?}

Installing a web browser, an extension for it, and finally a custom script
can be a bit complicated for certain users. We have to expect the
test users to be a selection of our general population and some could
therefore have trouble with achieving such a setup. On this aspect
a solution that alters the original application would work better
since users are not required to change their computing environment.
We hope to partly solve this problem with providing clear instructions
for how users can configure their environments to support custom
Greasemonkey scripts.

\subsubsection{What size is the user population that needs the new feature?}

This factor is based around the fact that server applications can be more
easily updated since they seldom require intervention by the user. It's
much harder to ensure that client applications like Greasemonkey are
kept up-to-date by it's users. It's argued that such an approach works
better for smaller populations and one should therefore keep users
of custom Greasemonkey scripts to a minimum.

Since we're not foreseeing the use of our implementation outside user
testing the application does not have to be kept up to date. In addition
we're anticipating a fairly limited user base due to our test setting.
Greasemonkey should therefore not put any hindrance in place for
our implementation in relation to this factor.

\subsubsection{Is the application exposed to the public internet?}

This factor relates to the previous about keeping the user base limited.
In addition to limiting the size of your user population it's also
important to know them\dash{}know who your users are\dash{}so that
one more easily can support them if need be. Since our application
only will be available to test users we see no disadvantage to
using Greasemonkey in this regard.

\subsubsection{How often is the page structure in the web application
  changing?}

Since implementations based around Greasemonkey often is dependent on the
underlying web site and it's structure it's important that this remains
stable while the implementation is in use. While we in our project
will have a fairly small window in time of use we've been concerned
that changes could break our implementation. This is especially true
for our data collection implementation which relies very much on the structure
of the \urort{} web site.

We see to possible solutions for this problematic aspect of Greasemonkey
implementations. Firstly communication between the developers of
\urort{} and us about upcoming changes would allow us to anticipate
them and handle them gracefully when they arrive. Secondly one could
introduce a meta structure in the web site by using agreed-upon
class names of certain \abbr{HTML} elements in the style of
what \term{microformats}%
\sidenote{
  Microformats are a set of simple and open data formats for better
  structuring of web content. All microformats adheres to the
  principles of solving a specific problem, starting as simply as
  possible, being designed for humans first\dash{}machines second,
  reusing elements from already established standards
  being modular and embeddable, and encouraging decentralized development,
  content, and services \citep[\p{7}]{allsopp07}.
  To us the essence of microformats seems to be the usage of semantic class
  names in \abbr{HTML} elements. By semantic class names we mean naming
  classes for what the elements they belong to represent.
}
are trying to achieve.
% discuss this further here or another place

\subsubsection{Does the application manage or expose sensitive information?}

Because of the architecture of Greasemonkey, scripts written for the extension
can be vulnerable to attacks if the developer is not careful. It's
therefore not adviceable to use Greasemonkey for highly sensitive web sites.
This is not a concern for our project as all the information handled by our
application can be found publicly and be viewed by all on the \urort{}
web site.

\subsubsection{Does the new feature require many changes to the existing web
  application?}

Using Greasemonkey to enhance and alter existing web pages is not as straight
forward as altering the source of the web page. Therefore massive changes to
an existing web site is best handled directly in the source code leaving
Greasemonkey to be best when only smaller alterations and addition are needed.

The changes we propose to introduce in the \urort{} web site is not earth
shattering in scope and size. We therefore believe and hope our implementation
can be implemented without too much additional effort with Greasemonkey.

\subsubsection{Does the target web application already have JavaScript code
  that mutates the page?}

This factor tries to capture the fact that the behavior implemented with
custom Greasemonkey scripts are run before any additional behavior
implemented in the web site itself. It's therefore hard to manipulate
a web site that mutates over time.

In our project we seem to be quite fortunate as we're only planning on adding
behavior to the \urort{} web site, not changing any existing behavior.
Therefore we don't have to concern ourselves with some of the dynamically
already present on \urort{}.

\subsubsection{Does the feature require communication with a server in a
  different network domain than the web application?}

Standard web pages can asynchronously retrieve information trough using
JavaScript and the \code{XMLHttpRequest} object. To enforce a certain
level of security browsers does not allow such requests to retrieve
information from other domains than what the request was sent from.
This limitation is eliminated in Greasemonkey scripts as one can request
information from other domains trough a similar asynchronous request object
existing in the extension.

This feature is vital for our project. As we'll see in the next section we're
dependent on requesting information from a resource that we ourself have
control over from the custom Greasemonkey script. This resource is however
not hosted in the domain that \urort{} is operating within. Without
cross-domain requests in Greasemonkey our implementation would be infeasible.

Based on how our project positioned itself with regard to the important
factors for the feasibility of using Greasemonkey
according to \citet{laird07}, we have to say that we
don't see any major objections for doing so.
We believe the benefits of a prototype implementation
based on Greasemonkey outweighs the disadvantages with such a technical
solution.

\subsection{Client-Server Model}

A client-server model is an architecture where one is separating a system into
two logical parts: a client and a server \citep[\p{3}]{lewandowski98}. The
client is a \openpostquote[\p{11}]{malkin96}{%
  computer system or process that requests a service of another
  computer system or process}
and the server is a \postquote[\p{49}]{malkin96}{%
  provider of resources}
They have disparate responsibilities, the client is a consumer and the
server is a producer \citep[\p{3}]{lewandowski98}. This delegation is the
essential part of client-server computing and enables one to focus on one
aspect of a problem as one does when adhering to the concept of
\term{separation of concerns} \citep[\p{61}]{dijkstra82}.
A client-server model also enables one to scale both horizontally and
vertically%
\sidenote{
  To scale horizontally entails adding more servers to a client-server
  architecture. When one increases the resources of a single server
  one is scaling vertically.
},
an impossible feat with monolithic systems \citep[\pp{7}{8}]{lewandowski98}.

We therefore decided to use a client-server architecture so that we could
offload some of the more computationally expensive operations off the client
and onto a dedicated server. Another benefit of such an architecture is that
it allows us to cache data globally\dash{}shared by all clients. This means
that data collection is handled on the server-side, while data display
obviously is handled on the client-side.

\citet[\p{887}--888]{nishimoto06} implemented a system for re-finding places one
have already visited on the Web. What's interesting about their system is that
their architecture is strikingly similar to our own. They use a
client-server model with a user-script enabled browser as their client. The
client facilitates the users as they're navigating by supplying additional
information alongside existing web pages. One of the ingredients in helping
users in their browsing is suppling information from third party content
providers. By separating this computationally heavy part of the application
into the server-side they were able to offload the clients in a similar
manner we intended when fetching data from \urort{}.

\subsection{Persistence}
In the planning stages of our implementation and it's early development we
decided to store all information retrieved from the \urort{} web site
in a relational database. We did some study into selecting the best
\abbr{ORM}%
\sidenote{
  \abbr{ORM}, an acronym for object-relational mapping, is
  \postquote[\p{889}]{linskey07}{%
    the technique of converting records in a relational database into
    object instances in an object-oriented programming environment}
  This is possible since \postquote[\p{2}]{marshall07}{%
    relational databases can be represented reasonably in object-based code if
    you simply think of database tables as classes, table rows as objects, and
    table fields as object attributes}

}
for connecting our application to the underlying database. We had this nice
model where the underlying relational database engine was abstracted away
and interaction to this database was done with a clear and explicative
\abbr{DSL}%
\sidenote{
  \abbr{DSL} or domain-specific languages are programming languages shaped
  for a specific domain. By doing so one can offer more expressiveness in a
  limited application domain. This results in better usability
  and increased productivity within the limited domain compared to
  using a general-purpose programming language \citep[\p{317}]{mernik05}.
},
eliminating the need for constructing complex \abbr{SQL}.%
\sidenote{
  \abbr{SQL}, (initially named \abbr{SEQUEL}) stands for structured
  query language, now the \latin{de facto} language for interacting
  with relational databases, invented by 
  \citet[\p{250}]{chamberlin74}.
}.

Then we had a sudden realization when coding on the part of our application
that retrieved information from the external \urort{} web site, pushed it into
our relational database, and made our data available in a structured manner
trough our \abbr{ORM} layer. It was not critical if we lost some or the
entirety of this data. We could just retrieve it again from our external
source. And since the data had to be kept up to date by retrieving the data
from our source within certain intervals we saw no need to make it available
in a relational database. We were not trying to persist data, but rather
keeping a cache of what we had retrieved.


\subsection{Caching}
% write about the cache and what is done on a \term{cache miss}. Also give
% reasons for \term{time to live}.
As described we came to the realization that no persistent data was needed
in our implementation.
% maybe user preferences? could be stored as cookies
We therefore sat out to find the best cache solution for our needs as detailed
in \sectionref{selection.stack.server.cache}.
% write more about details of implementation, cache terms, our settings,
% and custom cacheable object we implemented.

\subsection{Asynchronous Requests}

In the traditional style of \abbr{AJAX} applications we request information on
the client-side of our prototype asynchronous%
\sidenote{
  While it's possible to use synchronous requests with \code{XMLHttpRequest}
  it's strongly discouraged since the entire web browser will be locked
  while it's waiting for a response for it's request \citep{crockford06a}.
}.
This means that requests handled by the \code{XMLHttpRequest} object
are independent of other requests the browser is making\dash{}they
are non-blocking. This enables developers to create web pages where additional
data is requested after the page is loaded by a normal \abbr{HTTP} request.
This is often coupled with the ability to detect user behavior, and contents
are requested only when needed. \citet[pp.281--282]{stamey06} argues that
the increased complexity (and thereby size) of todays web pages have created
problems in perceived responsiveness for users due to network latency. He
thinks asynchronous request of small content items solves this
problem\dash{}bringing greater interactivity to the user. Another solution
that can be combined is to move some processing into the client with
JavaScript \citep[\p{9}]{jazayeri07}

\subsection{Modularization}

Both our client-side and server-side programming languages are as we'll see in
\sectionref{selection.stack.client.language} and
\sectionref{selection.stack.server.language} \term{object-oriented}%
\sidenote{
  The term object-orientation was coined by \citet{kay03} sometime in 1967.
  The motivations leading to object-oriented programming were mainly
  to find \q{%
    a better module scheme for complex systems involving hiding of details}
  and finding \postquote[p{514}]{kay96}{%
    a more flexible version of assignment, and then try to
    eliminate it altogether}
  The essence of object-orientation lies in the fundamental concepts of
  abstraction, classes, encapsulation, inheritance, object, message passing,
  methods, and polymorphism \citep[\pp{124}{126}]{armstrong06}.
}.

% ruby modules, classes, separate files and directories
% maybe MVC: reenskaug79
%
% the purpose of abstracting is not to be vague, but to create a new semantic
% level in which one can be absolutely precise.
%   -- Edsger W. Dijkstra, The Humble Programmer

\subsection{Data Structure}
% the structure of our persistent data

\subsection{Packaging}
% packaged up with rubygems to make exchange and installation of the software
% easier.
%
% javascript packaging necessary? only downloaded once. gzip compression not
% an option. maybe minifying or deep packaging processes?
%
% rake task for concatenating together javascript source in /ext/
% maybe fetch jQuery from net with open-uri
% have a version constant for easy fetching of required jQuery

\section{Performance}
% we did not design the application with performance in mind.
% cite Donald Knuth Structured Programming with GO TO Statements
